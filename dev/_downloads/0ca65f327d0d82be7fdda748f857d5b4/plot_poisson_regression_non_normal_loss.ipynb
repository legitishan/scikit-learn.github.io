{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Poisson regression and non-normal loss\n\n\nThis example illustrates the use of log-linear Poisson regression\non the `French Motor Third-Party Liability Claims dataset\n<https://www.openml.org/d/41214>`_ from [1]_ and compares\nit with models learned with least squared error. In this dataset, each sample\ncorresponds to an insurance policy, i.e. a contract within an insurance\ncompany and an individual (policiholder). Available features include driver\nage, vehicle age, vehicle power, etc.\n\nA few definitions: a *claim* is the request made by a policyholder to the\ninsurer to compensate for a loss covered by the insurance. The *exposure* is\nthe duration of the insurance coverage of a given policy, in years.\n\nOur goal is to predict the expected number of insurance claims (or frequency)\nfollowing car accidents for a policyholder given the historical data over a\npopulation of policyholders.\n\n.. [1]  A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor\n    Third-Party Liability Claims (November 8, 2018).\n    `doi:10.2139/ssrn.3164764 <http://dx.doi.org/10.2139/ssrn.3164764>`_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\n# Authors: Christian Lorentzen <lorentzen.ch@gmail.com>\n#          Roman Yurchak <rth.yurchak@gmail.com>\n# License: BSD 3 clause\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import Ridge, PoissonRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.utils import gen_even_slices\nfrom sklearn.metrics import auc\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import mean_poisson_deviance\n\n\ndef load_mtpl2(n_samples=100000):\n    \"\"\"Fetch the French Motor Third-Party Liability Claims dataset.\n\n    Parameters\n    ----------\n    n_samples: int or None, default=100000\n      Number of samples to select (for faster run time). If None, the full\n      dataset with 678013 samples is returned.\n    \"\"\"\n\n    # freMTPL2freq dataset from https://www.openml.org/d/41214\n    df = fetch_openml(data_id=41214, as_frame=True)['data']\n\n    # unquote string fields\n    for column_name in df.columns[df.dtypes.values == np.object]:\n        df[column_name] = df[column_name].str.strip(\"'\")\n    if n_samples is not None:\n        return df.iloc[:n_samples]\n    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the motor claim dataset. We ignore the severity data for this\nstudy for the sake of simplicitly.\n\nWe also subsample the data for the sake of computational cost and running\ntime. Using the full dataset would lead to similar conclusions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = load_mtpl2(n_samples=300000)\n\n# Correct for unreasonable observations (that might be data error)\ndf[\"Exposure\"] = df[\"Exposure\"].clip(upper=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The remaining columns can be used to predict the frequency of claim events.\nThose columns are very heterogeneous with a mix of categorical and numeric\nvariables with different scales, possibly very unevenly distributed.\n\nIn order to fit linear models with those predictors it is therefore\nnecessary to perform standard feature transformations as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_scale_transformer = make_pipeline(\n    FunctionTransformer(np.log, validate=False),\n    StandardScaler()\n)\n\nlinear_model_preprocessor = ColumnTransformer(\n    [\n        (\"passthrough_numeric\", \"passthrough\",\n            [\"BonusMalus\"]),\n        (\"binned_numeric\", KBinsDiscretizer(n_bins=10),\n            [\"VehAge\", \"DrivAge\"]),\n        (\"log_scaled_numeric\", log_scale_transformer,\n            [\"Density\"]),\n        (\"onehot_categorical\", OneHotEncoder(),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n    ],\n    remainder=\"drop\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of claims (``ClaimNb``) is a positive integer that can be modeled\nas a Poisson distribution. It is then assumed to be the number of discrete\nevents occurring with a constant rate in a given time interval\n(``Exposure``, in units of years). Here we model the frequency\n``y = ClaimNb / Exposure``, which is still a (scaled) Poisson distribution,\nand use ``Exposure`` as ``sample_weight``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n\nprint(\n   pd.cut(df[\"Frequency\"], [-1e-6, 1e-6, 1, 2, 3, 4, 5]).value_counts()\n)\n\nprint(\"Average Frequency = {}\"\n      .format(np.average(df[\"Frequency\"], weights=df[\"Exposure\"])))\n\nprint(\"Percentage of zero claims = {0:%}\"\n      .format(df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() /\n              df[\"Exposure\"].sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is worth noting that 92 % of policyholders have zero claims, and if we\nwere to convert this problem into a binary classification task, it would be\nsignificantly imbalanced.\n\nTo evaluate the pertinence of the used metrics, we will consider as a\nbaseline a \"dummy\" estimator that constantly predicts the mean frequency of\nthe training sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, random_state=0)\n\ndummy = make_pipeline(\n    linear_model_preprocessor,\n    DummyRegressor(strategy='mean')\n)\ndummy.fit(df_train, df_train[\"Frequency\"],\n          dummyregressor__sample_weight=df_train[\"Exposure\"])\n\n\ndef score_estimator(estimator, df_test):\n    \"\"\"Score an estimator on the test set.\"\"\"\n\n    y_pred = estimator.predict(df_test)\n\n    print(\"MSE: %.3f\" %\n          mean_squared_error(df_test[\"Frequency\"], y_pred,\n                             df_test[\"Exposure\"]))\n    print(\"MAE: %.3f\" %\n          mean_absolute_error(df_test[\"Frequency\"], y_pred,\n                              df_test[\"Exposure\"]))\n\n    # ignore non-positive predictions, as they are invalid for\n    # the Poisson deviance\n    mask = y_pred > 0\n    if (~mask).any():\n        warnings.warn(\"Estimator yields non-positive predictions for {} \"\n                      \"samples out of {}. These will be ignored while \"\n                      \"computing the Poisson deviance\"\n                      .format((~mask).sum(), mask.shape[0]))\n\n    print(\"mean Poisson deviance: %.3f\" %\n          mean_poisson_deviance(df_test[\"Frequency\"][mask],\n                                y_pred[mask],\n                                df_test[\"Exposure\"][mask]))\n\n\nprint(\"Constant mean frequency evaluation:\")\nscore_estimator(dummy, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by modeling the target variable with the least squares linear\nregression model,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ridge = make_pipeline(linear_model_preprocessor, Ridge(alpha=1.0))\nridge.fit(df_train, df_train[\"Frequency\"],\n          ridge__sample_weight=df_train[\"Exposure\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Poisson deviance cannot be computed on non-positive values predicted by\nthe model. For models that do return a few non-positive predictions\n(e.g. :class:`linear_model.Ridge`) we ignore the corresponding samples,\nmeaning that the obtained Poisson deviance is approximate. An alternative\napproach could be to use :class:`compose.TransformedTargetRegressor`\nmeta-estimator to map ``y_pred`` to a strictly positive domain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Ridge evaluation:\")\nscore_estimator(ridge, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we fit the Poisson regressor on the target variable. We set the\nregularization strength ``alpha`` to 1 over number of samples in oder to\nmimic the Ridge regressor whose L2 penalty term scales differently with the\nnumber of samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "poisson = make_pipeline(\n    linear_model_preprocessor,\n    PoissonRegressor(alpha=1/df_train.shape[0], max_iter=1000)\n)\npoisson.fit(df_train, df_train[\"Frequency\"],\n            poissonregressor__sample_weight=df_train[\"Exposure\"])\n\nprint(\"PoissonRegressor evaluation:\")\nscore_estimator(poisson, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we will consider a non-linear model, namely a random forest. Random\nforests do not require the categorical data to be one-hot encoded: instead,\nwe can encode each category label with an arbitrary integer using\n:class:`preprocessing.OrdinalEncoder`. With this encoding, the forest will\ntreat the categorical features as ordered features, which might not be always\na desired behavior. However this effect is limited for deep enough trees\nwhich are able to recover the categorical nature of the features. The main\nadvantage of the :class:`preprocessing.OrdinalEncoder` over the\n:class:`preprocessing.OneHotEncoder` is that it will make training faster.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rf_preprocessor = ColumnTransformer(\n    [\n        (\"categorical\", OrdinalEncoder(),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n        (\"numeric\", \"passthrough\",\n            [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]),\n    ],\n    remainder=\"drop\",\n)\nrf = make_pipeline(\n    rf_preprocessor,\n    RandomForestRegressor(min_weight_fraction_leaf=0.01, n_jobs=2)\n)\nrf.fit(df_train, df_train[\"Frequency\"].values,\n       randomforestregressor__sample_weight=df_train[\"Exposure\"].values)\n\n\nprint(\"RandomForestRegressor evaluation:\")\nscore_estimator(rf, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like the Ridge regression above, the random forest model minimizes the\nconditional squared error, too. However, because of a higher predictive\npower, it also results in a smaller Poisson deviance than the Poisson\nregression model.\n\nEvaluating models with a single train / test split is prone to random\nfluctuations. If computing resources allow, it should be verified that\ncross-validated performance metrics would lead to similar conclusions.\n\nThe qualitative difference between these models can also be visualized by\ncomparing the histogram of observed target values with that of predicted\nvalues:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 4, figsize=(16, 6), sharey=True)\nfig.subplots_adjust(bottom=0.2)\nn_bins = 20\nfor row_idx, label, df in zip(range(2),\n                              [\"train\", \"test\"],\n                              [df_train, df_test]):\n    df[\"Frequency\"].hist(bins=np.linspace(-1, 30, n_bins),\n                         ax=axes[row_idx, 0])\n\n    axes[row_idx, 0].set_title(\"Data\")\n    axes[row_idx, 0].set_yscale('log')\n    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n    axes[row_idx, 0].set_ylim([1e1, 5e5])\n    axes[row_idx, 0].set_ylabel(label + \" samples\")\n\n    for idx, model in enumerate([ridge, poisson, rf]):\n        y_pred = model.predict(df)\n\n        pd.Series(y_pred).hist(bins=np.linspace(-1, 4, n_bins),\n                               ax=axes[row_idx, idx+1])\n        axes[row_idx, idx + 1].set(\n            title=model[-1].__class__.__name__,\n            yscale='log',\n            xlabel=\"y_pred (predicted expected Frequency)\"\n        )\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The experimental data presents a long tail distribution for ``y``. In all\nmodels we predict a mean expected value, so we will have necessarily fewer\nextreme values. Additionally, the normal distribution used in ``Ridge`` and\n``RandomForestRegressor`` has a constant variance, while for the Poisson\ndistribution used in ``PoissonRegressor``, the variance is proportional to\nthe mean predicted value.\n\nThus, among the considered estimators, ``PoissonRegressor`` is better suited\nfor modeling the long tail distribution of the data as compared to the\n``Ridge`` and ``RandomForestRegressor`` estimators.\n\nTo ensure that estimators yield reasonable predictions for different\npolicyholder types, we can bin test samples according to ``y_pred`` returned\nby each model. Then for each bin, we compare the mean predicted ``y_pred``,\nwith the mean observed target:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None,\n                                  n_bins=100):\n    \"\"\"Compare predictions and observations for bins ordered by y_pred.\n\n    We order the samples by ``y_pred`` and split it in bins.\n    In each bin the observed mean is compared with the predicted mean.\n\n    Parameters\n    ----------\n    y_true: array-like of shape (n_samples,)\n        Ground truth (correct) target values.\n    y_pred: array-like of shape (n_samples,)\n        Estimated target values.\n    sample_weight : array-like of shape (n_samples,)\n        Sample weights.\n    n_bins: int\n        Number of bins to use.\n\n    Returns\n    -------\n    bin_centers: ndarray of shape (n_bins,)\n        bin centers\n    y_true_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    y_pred_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    \"\"\"\n    idx_sort = np.argsort(y_pred)\n    bin_centers = np.arange(0, 1, 1/n_bins) + 0.5/n_bins\n    y_pred_bin = np.zeros(n_bins)\n    y_true_bin = np.zeros(n_bins)\n\n    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n        weights = sample_weight[idx_sort][sl]\n        y_pred_bin[n] = np.average(\n            y_pred[idx_sort][sl], weights=weights\n        )\n        y_true_bin[n] = np.average(\n            y_true[idx_sort][sl],\n            weights=weights\n        )\n    return bin_centers, y_true_bin, y_pred_bin\n\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 3.5))\nplt.subplots_adjust(wspace=0.3)\n\nfor axi, model in zip(ax, [ridge, poisson, rf]):\n    y_pred = model.predict(df_test)\n\n    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n        df_test[\"Frequency\"].values,\n        y_pred,\n        sample_weight=df_test[\"Exposure\"].values,\n        n_bins=10)\n\n    axi.plot(q, y_pred_seg, marker='o', linestyle=\"-\", label=\"predictions\")\n    axi.plot(q, y_true_seg, marker='x', linestyle=\"--\", label=\"observations\")\n    axi.set_xlim(0, 1.0)\n    axi.set_ylim(0, 0.6)\n    axi.set(\n        title=model[-1].__class__.__name__,\n        xlabel='Fraction of samples sorted by y_pred',\n        ylabel='Mean Frequency (y_pred)'\n    )\n    axi.legend()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``Ridge`` regression model can predict very low expected frequencies\nthat do not match the data. It can therefore severly under-estimate the risk\nfor some policyholders.\n\n``PoissonRegressor`` and ``RandomForestRegressor`` show better consistency\nbetween predicted and observed targets, especially for low predicted target\nvalues.\n\nHowever, for some business applications, we are not necessarily interested\nin the ability of the model to predict the expected frequency value, but\ninstead to predict which policyholder groups are the riskiest and which are\nthe safest. In this case, the model evaluation would cast the problem as a\nranking problem rather than a regression problem.\n\nTo compare the 3 models within this perspective, one can plot the fraction of\nthe number of claims vs the fraction of exposure for test samples ordered by\nthe model predictions, from safest to riskiest  according to each model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _cumulated_claims(y_true, y_pred, exposure):\n    idx_sort = np.argsort(y_pred)  # from safest to riskiest\n    sorted_exposure = exposure[idx_sort]\n    sorted_frequencies = y_true[idx_sort]\n    cumulated_exposure = np.cumsum(sorted_exposure)\n    cumulated_exposure /= cumulated_exposure[-1]\n    cumulated_claims = np.cumsum(sorted_exposure * sorted_frequencies)\n    cumulated_claims /= cumulated_claims[-1]\n    return cumulated_exposure, cumulated_claims\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor model in [ridge, poisson, rf]:\n    y_pred = model.predict(df_test)\n    cum_exposure, cum_claims = _cumulated_claims(\n        df_test[\"Frequency\"].values,\n        y_pred,\n        df_test[\"Exposure\"].values)\n    area = auc(cum_exposure, cum_claims)\n    label = \"{} (area under curve: {:.3f})\".format(\n        model[-1].__class__.__name__, area)\n    ax.plot(cum_exposure, cum_claims, linestyle=\"-\", label=label)\n\n# Oracle model: y_pred == y_test\ncum_exposure, cum_claims = _cumulated_claims(\n    df_test[\"Frequency\"].values,\n    df_test[\"Frequency\"].values,\n    df_test[\"Exposure\"].values)\narea = auc(cum_exposure, cum_claims)\nlabel = \"Oracle (area under curve: {:.3f})\".format(area)\nax.plot(cum_exposure, cum_claims, linestyle=\"-.\", color=\"gray\", label=label)\n\n# Random Baseline\nax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\",\n        label=\"Random baseline\")\nax.set(\n    title=\"Cumulated number of claims by model\",\n    xlabel='Fraction of exposure (from safest to riskiest)',\n    ylabel='Fraction of number of claims'\n)\nax.legend(loc=\"upper left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot reveals that the random forest model is slightly better at ranking\npolicyholders by risk profiles even if the absolute value of the predicted\nexpected frequencies are less well calibrated than for the linear Poisson\nmodel.\n\nAll three models are significantly better than chance but also very far from\nmaking perfect predictions.\n\nThis last point is expected due to the nature of the problem: the occurrence\nof accidents is mostly dominated by circumstantial causes that are not\ncaptured in the columns of the dataset or that are indeed random.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}